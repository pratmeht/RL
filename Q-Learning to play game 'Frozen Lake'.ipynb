{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Description of the game \"Frozen Lake\"\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "The surface is described using a grid like the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SFFF       (S: starting point, safe)\n",
    "FHFH       (F: frozen surface, safe)\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "HFFG       (G: goal, where the frisbee is located)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gym.openai.com/envs/FrozenLake-v0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gym.wrappers.time_limit.TimeLimit"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will construct the Q-Table and initialize all the Q-values to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create and initialize all the parameters needed to implement the Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********* Average Reward per thousand episodes *********\n",
      "\n",
      "1000 : 0.6750000000000005\n",
      "2000 : 0.6740000000000005\n",
      "3000 : 0.6670000000000005\n",
      "4000 : 0.6950000000000005\n",
      "5000 : 0.7010000000000005\n",
      "6000 : 0.6520000000000005\n",
      "7000 : 0.6750000000000005\n",
      "8000 : 0.6670000000000005\n",
      "9000 : 0.7040000000000005\n",
      "10000 : 0.6870000000000005\n",
      "\n",
      "\n",
      "********** Q-table ***********\n",
      "\n",
      "\n",
      "[[0.55616189 0.51399505 0.51051543 0.52904098]\n",
      " [0.37103296 0.33522746 0.30874892 0.4967635 ]\n",
      " [0.42479016 0.42368968 0.40863647 0.46898985]\n",
      " [0.23433073 0.32958115 0.27589545 0.45005899]\n",
      " [0.57052725 0.43469314 0.36495402 0.42802347]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.18340733 0.1678329  0.40207604 0.16206023]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.37940093 0.31980009 0.43297366 0.60365794]\n",
      " [0.477043   0.65009997 0.46648155 0.38779166]\n",
      " [0.61797683 0.35795662 0.3787674  0.40144973]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.51737766 0.55179143 0.7365094  0.60571987]\n",
      " [0.7319281  0.83095998 0.73091351 0.71337093]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rewards_all_episodes = list()\n",
    "\n",
    "# Q-Learning Algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        \n",
    "        # Exploration-Exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-Table for Q(s,a)\n",
    "        q_table[state,action] = q_table[state,action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "                \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "    # Exploration Rate decay \n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)\n",
    "    \n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "    \n",
    "# Calculate and print the average reward per 1000 episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)\n",
    "count = 1000\n",
    "                                 \n",
    "print(\"********* Average Reward per thousand episodes *********\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \":\", str(sum(r/1000)))\n",
    "    count += 1000\n",
    "                                \n",
    "# Print updated Q-table\n",
    "print(\"\\n\\n********** Q-table ***********\\n\\n\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have trained our model. Now it is a time to see our model play the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will watch 3 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "*****You fell in the hole******\n"
     ]
    }
   ],
   "source": [
    "for episode in range(3):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print('***** Episode ', episode+1 ,'*****\\n\\n\\n\\n')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"*****You reached the goal******\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print('*****You fell in the hole******')\n",
    "                time.sleep(3)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "        \n",
    "env.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
